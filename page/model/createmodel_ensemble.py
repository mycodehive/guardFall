import streamlit as st
import numpy as np
import pandas as pd
import os, time
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Dropout, LSTM, Input, Concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import joblib
import matplotlib.pyplot as plt

def show(auto_run):
    if auto_run :
        st.title("ğŸ§  Dense + LSTM ì•™ìƒë¸” ëª¨ë¸ ìƒì„±")
        st.write("ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‘ ê°œì˜ ëª¨ë¸(Dense, LSTM)ì„ í•™ìŠµí•˜ê³  ì•™ìƒë¸”í•©ë‹ˆë‹¤.")

        filename = "fall_model_ensembleModel.keras"
        pkl_filename = "fall_model_ensembleModel.pkl"

        displaytime = 1.5
        progress_box = st.empty()
        csv_dir = os.path.abspath(os.path.join("user", "csv"))
        csv_files = [f for f in os.listdir(csv_dir) if f.endswith(".csv") and f != "merged_user_data.csv"]

        time.sleep(displaytime)
        progress_box.success("âœ”ï¸ íŒŒì¼ì„ ë³‘í•© ì¤‘ì…ë‹ˆë‹¤...")

        df_list = [pd.read_csv(os.path.join(csv_dir, f)) for f in csv_files]
        st.code("\n".join(csv_files))
        time.sleep(displaytime)
        merged_df = pd.concat(df_list, ignore_index=True)
        progress_box.success(f"âœ”ï¸ ì´ {len(csv_files)}ê°œì˜ CSV íŒŒì¼ì„ í•©ì³ì„œ í•˜ë‚˜ì˜ ë°ì´í„°ë¡œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.")

        time.sleep(displaytime)
        filtered_df = merged_df[merged_df["checkFall"] == 1]
        total_falls = filtered_df.shape[0]
        st.info(f"âœ”ï¸ ì „ì²´ ë°ì´í„° ì¤‘ ë‚™ìƒìœ¼ë¡œ íŒë³„ëœ ë°ì´í„°({total_falls}ê±´)ë§Œ ë³´ê¸° ì™„ë£Œ")
        st.dataframe(filtered_df)

        time.sleep(displaytime)
        csv_path = os.path.join(csv_dir, "merged_user_data.csv")
        merged_df.to_csv(csv_path, index=False)
        progress_box.success("âœ”ï¸ CSV ë³‘í•© íŒŒì¼ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

        df = pd.read_csv(csv_path)

        time.sleep(displaytime)
        progress_box.success("âœ”ï¸ [1] ì…ë ¥(X)ê³¼ íƒ€ê²Ÿ(y)ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.")
        X = df.drop(columns=['timestamp', 'checkFall'])
        y = df['checkFall']

        time.sleep(displaytime)
        progress_box.success("âœ”ï¸ [2] ìŠ¤ì¼€ì¼ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        X_scaled_3d = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))

        time.sleep(displaytime)
        progress_box.success("âœ”ï¸ [3] í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        X_train_2d, X_test_2d, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42
        )
        X_train_3d = X_train_2d.reshape((X_train_2d.shape[0], 1, X_train_2d.shape[1]))
        X_test_3d = X_test_2d.reshape((X_test_2d.shape[0], 1, X_test_2d.shape[1]))

        time.sleep(displaytime)
        progress_box.success("âœ”ï¸ [4] ì•™ìƒë¸” ëª¨ë¸ì„ ì •ì˜í•©ë‹ˆë‹¤.")
        # Dense Branch
        input_dense = Input(shape=(X_train_2d.shape[1],))
        x1 = Dense(64, activation='relu')(input_dense)
        x1 = Dropout(0.3)(x1)
        x1 = Dense(32, activation='relu')(x1)
        x1 = Dropout(0.3)(x1)

        # LSTM Branch
        input_lstm = Input(shape=(X_train_3d.shape[1], X_train_3d.shape[2]))
        x2 = LSTM(64, return_sequences=False)(input_lstm)
        x2 = Dropout(0.3)(x2)

        # Concatenate
        merged = Concatenate()([x1, x2])
        x = Dense(32, activation='relu')(merged)
        x = Dropout(0.3)(x)
        output = Dense(1, activation='sigmoid')(x)
        model = Model(inputs=[input_dense, input_lstm], outputs=output)

        time.sleep(displaytime)
        progress_box.success("âœ”ï¸ [5] ëª¨ë¸ì„ ì»´íŒŒì¼í•©ë‹ˆë‹¤.")
        model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

        time.sleep(displaytime)
        progress_box.success("âœ”ï¸ [6] EarlyStoppingì„ ì ìš©í•©ë‹ˆë‹¤.")
        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

        time.sleep(displaytime)
        progress_box.warning("âœ”ï¸ [7] ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤. ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë¦½ë‹ˆë‹¤. ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
        history = model.fit(
            [X_train_2d, X_train_3d], y_train,
            validation_data=([X_test_2d, X_test_3d], y_test),
            epochs=100,
            batch_size=32,
            callbacks=[early_stopping],
            verbose=1
        )

        model_dir = os.path.abspath(os.path.join("user", "model"))
        os.makedirs(model_dir, exist_ok=True)

        model_save_path = os.path.join(model_dir, filename)
        model.save(model_save_path)
        progress_box.success("âœ”ï¸ [8] ëª¨ë¸ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

        scaler_save_path = os.path.join(model_dir, pkl_filename)
        joblib.dump(scaler, scaler_save_path)
        progress_box.success("âœ”ï¸ [9] ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

        loss, accuracy = model.evaluate([X_test_2d, X_test_3d], y_test, verbose=0)
        progress_box.success("âœ”ï¸ [10] í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì •í™•ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.")

        col1, col2 = st.columns(2)
        with col1:
            fig_acc, ax_acc = plt.subplots(figsize=(6, 4))
            ax_acc.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)
            ax_acc.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)
            ax_acc.set_title('Model Accuracy Curve')
            ax_acc.set_xlabel('Epoch')
            ax_acc.set_ylabel('Accuracy')
            ax_acc.grid(True, linestyle='--', alpha=0.5)
            ax_acc.legend(loc='lower right')
            st.pyplot(fig_acc)

        with col2:
            fig_loss, ax_loss = plt.subplots(figsize=(6, 4))
            ax_loss.plot(history.history['loss'], label='Train Loss', linewidth=2)
            ax_loss.plot(history.history['val_loss'], label='Val Loss', linewidth=2)
            ax_loss.set_title('Loss Over Epochs')
            ax_loss.set_xlabel('Epoch')
            ax_loss.set_ylabel('Loss')
            ax_loss.grid(True, linestyle='--', alpha=0.5)
            ax_loss.legend(loc='upper right')
            st.pyplot(fig_loss)

        acc_train_end = history.history['accuracy'][-1]
        acc_val_end = history.history['val_accuracy'][-1]
        acc_gap = abs(acc_train_end - acc_val_end)

        if acc_gap < 0.05:
            st.info("âœ… í•™ìŠµê³¼ ê²€ì¦ ì •í™•ë„ì˜ ì°¨ì´ê°€ ì‘ì•„ ì¼ë°˜í™” ì„±ëŠ¥ì´ ìš°ìˆ˜í•œ ëª¨ë¸ì…ë‹ˆë‹¤.")
        elif acc_train_end > acc_val_end:
            st.warning("âš ï¸ í•™ìŠµ ì •í™•ë„ëŠ” ë†’ì§€ë§Œ ê²€ì¦ ì •í™•ë„ê°€ ë‚®ì•„ ê³¼ì í•©ì´ ì˜ì‹¬ë©ë‹ˆë‹¤.")
        else:
            st.error("âŒ ì „ì²´ì ìœ¼ë¡œ ì •í™•ë„ê°€ ë‚®ì•„ ëª¨ë¸ êµ¬ì¡° ë˜ëŠ” ë°ì´í„° í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")

        #progress_box.info(f"ğŸ‰ ì•™ìƒë¸” ëª¨ë¸ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! '{model_dir}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì •í™•ë„ëŠ” {accuracy * 100:.2f}% ì…ë‹ˆë‹¤. ")
        progress_box.info(f"ğŸ‰ ì•™ìƒë¸” ëª¨ë¸ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! '{model_dir}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
